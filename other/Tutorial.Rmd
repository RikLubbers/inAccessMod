---
title: "inA"
author: "Anthony Lehmann"
date: "2022"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Objectif et instructions
L'objectif principal de ce TP est d'aborder les SDMs d'un point de vue pratique à travers R, et en utilisant comme exemple la modélisation de la distribution de l'espèce *Vulpes vulpes* (renard roux) à l'échelle mondiale en fonction de variables bioclimatiques. Vous trouverez ci-dessous une marche à suivre détaillée et des rappels théoriques. Il vous suffira de copier (ou réécrire) les morceaux de code indiqués dans R pour avancer et obtenir les 'outputs' correspondant à chaque étape. N'hésitez pas à interroger l'aide de R pour plus d'informations concernant les fonctions utilisées (ex. ?glm pour l'aide concernant la fonction 'glm'). Suivez les instructions en prenant régulièrement des saisies d'écran, notamment des 'outputs' (tableaux, graphiques, cartes, etc.), que vous collerez dans ce document. N'hésitez pas aussi à ajouter quelques commentaires utiles à votre compréhension. Ce mini-rapport servira de validation et sera à rendre avant le 31 mai pour valider votre participation à ce module. Si vous rendez 2 rapports acceptés dans le cadre du cours SPACE-ECOLOGY vous aurez la note 4, 3 rapports la note 5, et 4 rapports la note 6.

Pour en savoir plus au sujet des SDMs, vous pouvez consulter l'ouvrage suivant (disponible à l'Unige): 

*Guisan, A., Thuiller, W., & Zimmermann, N. (2017). Habitat Suitability and Distribution Models: With Applications in R (Ecology, Biodiversity and Conservation). Cambridge: Cambridge University Press. doi:10.1017/9781139028271*

C'est un livre très bien résumé et très complet qui aborde toutes les questions relatives aux modèles de distribution d'espèces, avec de nombreux exemples dans R.

Avant de commencer ce TP, lancez R studio, préparez une nouvelle page de script et exécutez le code ci-dessous. Il permettra d'installer (si besoin) et d'activer les librairies ('packages') requises.

```{r importPackage, message=FALSE, warning=FALSE}
# Installation des packages s'ils ne sont pas déjà installés.
if (!require("biomod2")) install.packages("biomod2")
if (!require("raster")) install.packages("raster")
if (!require("usdm")) install.packages("usdm")
if (!require("randomForest")) install.packages("randomForest")

# Activation des packages pour ce TP
library(biomod2) # Obtention des données, fonctions liées aux SDM
library(raster) # Traitement de données spatiales
library(usdm) # Traiter la multicolinéarité (fonction vifcor)
library(randomForest) # Fonction randomForest
```

Le TP sera divisé en six parties:

I) Préparation des données
II) Techniques de modélisation et calibration
III) Evaluation de l'importance relative de chaque variable explicative
IV) Evaluation des 'courbes réponses' associées à chaque variable explicative
V) Evaluation de la performance prédictive des modèles
VI) Prédictions à l'échelle globale et conclusion


### Partie I: Préparation des données

L'objectif d'un SDM est de mettre en relation une variable réponse (présence-absence d'une espèce codée en binaire 1/0) et des variables explicatives aussi appelés prédicteurs (variables environnementales). On a donc une **variable Y (espèce) reliée à un jeu de données environnementales X. Cette relation est définie par un modèle M**. Une fois le modèle calibré (défini, construit, etc.), on peut l'utiliser pour prédire Y' en fonction de X' (nouveau jeu de données environnementales). Dans cette partie nous allons voir comment préparer X et Y de telle manière à pouvoir calibrer M.   

Nous allons utiliser des données qui sont incluses dans le package 'biomod2', package spécialement dédié aux SDMs.

#### 1. Importation des données d'observations de *Vulpes vulpes* (variable réponse)

```{r importData1,message=FALSE,warning=FALSE}
# Importation d'un tableau csv, inclus dans le package biomod2
dataSpecies <- read.csv(system.file("external/species/mammals_table.csv",package="biomod2"), row.names = 1)
head(dataSpecies)
```

Ce tableau importé contient les coordonnées géographiques selon le système géodésique WGS84 (World Geodetic System 1984) de chaque observation, ainsi que plusieurs colonnes qui correspondent à différentes espèces. Nous allons extraire la colonne VulpesVulpes (la dernière) dont les valeurs sont binaires, 1 correspondant à une présence, 0 à une absence. Cette colonne sera ensuite utilisée pour créer notre tableau final qui servira à calibrer nos modèles.

```{r spp}
# Extraction de la colonne VulpesVulpes
spp <- dataSpecies[,"VulpesVulpes"]
```

#### 2. Données environnementales (variables explicatives)
Nous allons utiliser des données environnementales présentées sous forme de rasters qui correspondent à des variables bioclimatiques dérivées de variables de température et précipitation. Vous trouverez plus de détails concernant ces variables sur le site suivant: https://worldclim.org/data/bioclim.html 

Notez que les valeurs de températures ont été multipliées par 10 pour des questions de mémoire liées au stockage des données (https://worldclim.org/data/v1.4/formats.html).

Nous allons créer un 'stack' (une pile) de rasters. Ils seront 'superposés' et nous pourrons donc facilement obtenir la valeur des différentes variables à un point donné (à la manière d'une aiguille transperçant une pile de papier).

```{r importData2}
# La fonction stack permet d'empiler plusieurs rasters aux dimensions et résolutions identiques. Ces rasters proviennent du package biomod2.
dataEnv <- stack(system.file("external/bioclim/current/bio3.grd",
                             package="biomod2"),
                system.file("external/bioclim/current/bio4.grd",
                             package="biomod2"),
                system.file("external/bioclim/current/bio7.grd",
                             package="biomod2"),
                system.file("external/bioclim/current/bio11.grd",
                             package="biomod2"),
                system.file("external/bioclim/current/bio12.grd",
                             package="biomod2"))
# Affichage des rasters
plot(dataEnv)
```

#### 3. Obtenir les données environnementales pour chacune des observations faunistiques
Comme mentionné plus haut, nous allons extraire avec la fonction 'extract' les valeurs des rasters 'empilés' au niveau des points d'observations faunistiques.

```{r extract}
# Les deux premières colonnes du tableau dataSpecies correspondent aux coordonnées. Tappez ?extract pour plus d'informations concernant cette fonction.
envValues <- extract(x=dataEnv,y=dataSpecies[,c(1,2)])
# Si x avait été un seul raster, nous aurions un vecteur en sortie (une série de valeurs). Comme x est une pile de raster, nous avons un tableau en sortie (plusieurs séries disposées en colonnes). La fonction 'head' permet d'fficher les six premières lignes du tableau issu de la fonction extract. 
head(envValues)
# Plot permettant d'avoir un apperçu graphique des relations existantes entre ces variables.
plot(as.data.frame(envValues))
```

#### 4. Eliminer les variables colinéaires
Certaines techniques de modélisation (ex. régression) ne sont pas robustes si les variables explicatives sont très corrélées entre elles. Quoi qu'il en soit, utiliser des variables explicatives très corrélées (et donc redondantes) n'a pas vraiment de sens. C'est pourquoi il est préférable d'éliminer les variables qui pourraient causer un problème lié à leur colinéarité. Lorsque l'on a deux variables corrélées, le choix d'en garder une plutôt qu'une autre dépend en premier lieu de sa pertinence d'un point de vue écologique. En effet, on préfère souvent une variable dont l'impact sur l'espèce est plus direct. Par exemple, la température et l'altitude sont souvent très corrélées, mais pour des questions d'interprétations écologiques, il est plus judicieux de conserver la température plutôt que l'altitude. Une autre manière de choisir (celle que nous allons employer ici), c'est de se baser sur le VIF (Variation Inflation Factor) de chacune des variables. Le VIF donne une idée de la colinéarité d'une variable vis-à-vis des autres. En résumé, plus une variable peut être expliquée par les autres variables (à travers une régresssion linéaire multiple), plus son VIF est important. Lorsque l'on a une paire de variables dont la corrélation est élevée, il est donc recommandé de garder la variable qui a le VIF le plus bas (colinéarité plus faible vis-à-vis de toutes les autres variables). La fonction que nous allons utiliser ('vifcor') permettra, pour chaque paire de variables ayant un coefficient de corrélation supérieur à une seuil donné, de déterminer la variable à éliminer (VIF plus élevé). Le seuil est arbitraire, il est souvent de l'ordre de 0.7-0.8 lorsque l'on utilise des techniques de régression, mais dans notre cas, nous allons le définir à 0.9 (paramètre 'th' dans notre fonction). Nous allons donc ici, définir les variables explicatives (prédicteurs) qui seront utilisées pour nos modèles.      

```{r corr}
# Fonction vifcor pour évaluer la colinéarité des nos variables. Pour chaque paire de variables dont le coefficient de corrélation est égal ou supérieur à 0.9, la variable ayant le VIF le plus élevé sera mise de côté.
vifCorResults <- vifcor(envValues,th=0.9)
# Variable à éliminer: Le nom de ces variables se trouve sur le 'slot' (@) nommé 'excluded' (@excluded) de l'objet issu de la fonction vifcor.
varToRemove <- vifCorResults@excluded
# Variable à conserver: L'expression 'colnames(envValues)' donne le nom des colonnes de notre tableau de variables environnementales (c'est-à-dire le nom des variables explicatives). L'expression entre crochets '[!colnames(envValues) %in% varToRemove]' représente un vecteur logique (TRUE/FALSE) ('%in%' veut dire 'fait partie' et le point d'interrogation placé au début inverse le résultat). Cela donne TRUE quand le nom de la colonne ne fait pas partie de l'objet varToRemove, FALSE lorsqu'il en fait partie. L'objet 'preds' contiendra donc le nom des variables explicatives qui seront utilisées pour nos modèles.
preds <- colnames(envValues)[!colnames(envValues) %in% varToRemove]
print(preds)
```

#### 5. Tableau final
Nous allons créer un tableau final qui contiendra les observations faunistiques et les valeurs environnementales correspondantes retenues. Ce tableau nous servira comme base pour la calibration de nos modèles. 

```{r dataSpEnv}
# Nous joignons le vecteur de presence-absence de Vulpes vulpes (en lui donnant un nom; VulpesVulpes=...) et le tableau des valeurs environnementales issu de la fonction 'extract', tout en spécifiant les colonnes qui nous souhaitons conserver.
dataSpEnv <- data.frame(VulpesVulpes=spp,envValues[,preds])
head(dataSpEnv)
```


### Partie II: Techniques de modélisation et calibration

Il existe une multitude de techniques de modélisation (GLM, GAM, MaxENT, Random Forest, Gradient Boosted Trees, SVC, etc.), mais il serait impossible de toutes les aborder dans le cadre de cours. Nous allons néanmoins en voir deux, l'une étant une technique de régression (GLM) et l'autre une technique de 'machine learning' (Random Forest).  

#### 1. Régression: Generalized linear model (GLM)
Comme son nom l'indique, un GLM est une généralisation de la régression linéaire, permettant de modéliser des données qui ne sont pas normalement distribuées (loi normale). Dans notre cas, la variable réponse est binaire et suit une loi binomiale. Nous allons utiliser la fonction 'logit' comme fonction lien (voir théorie sur GLM).

![](logit.png)

La fonction 'glm' permet de calibrer un GLM de la manière suivante:

```{r glm}
# Calibration du GLM. Le nom de la variable réponse en fonction de toutes les variables (~.) contenues dans le tableau (data=). Nous spécifions aussi la famille de distribution (binomial) dont la fonction lien est la fonction 'logit'.
modGLM <- glm(VulpesVulpes~.,data=dataSpEnv,family = binomial("logit"))
# Summary
print(summary(modGLM))
```

Le 'summary' de notre modèle nous indique que tous nos coefficients ainsi que l'intercept (ordonnée à l'origine) sont significativement différents de zéro (Pr(>|z|) < 0.05). On peut dès lors inférer que toutes les variables retenues ont un effet significatif sur la probabilité de présence de *Vulpe vulpes*. L'AIC indiqué en-bas du 'summary' nous permet de comparer deux modèles construits sur les mêmes observations (la valeur en soit de l'AIC n'indique rien). Plus la valeur est faible, plus la qualité du modèle est élevée, comparée à celle de l'autre modèle.

Rappelons ici l'importance du compromis entre **biais et variance**. Le biais est l'erreur du modèle provenant de l'incapacité de l'algorithme à traduire le concept reliant la variable réponse aux prédicteurs (sous-apprentissage ou underfit en anglais). La variance est l'erreur due à la sensibilité aux petites fluctuations de l’échantillon utilisé pour la calibration (surapprentissage ou overfit en anglais). Dans ce cas, l'algorithme tend à modéliser le bruit et les modèles qui varient significativement selon l'échantillon utilisé pour la calibration ne peuvent être généralisés. Un des objectifs principaux en modélisation est de diminuer l'une des erreurs (biais ou variance) sans augmenter l'autre. L'AIC permet de choisir entre plusieurs GLMs, celui qui en même temps explique le mieux les valeurs observées tout en restant 'parcimonieux' et pouvant être généralisé. L'AIC ne peut être calculé sur des modèles non-paramétriques (tels que les Random Forest) et c'est pourquoi nous allons voir plus loin une méthode plus générale permettant de comparer la performance de modèles issus de technique différentes.

![](biasVariance.png)

#### 2. Machine Learning: Random Forest (RF)

Le Random Forest est une technique basée sur la combinaison d'arbres décisionnels. Un arbre décisionnel est un outil à la décision représentant un ensemble de choix sous la forme d'un arbre. La figure ci-dessous illustre comment l'interaction entre la température et les précipitations peuvent agir sur la présence (points verts) ou l'absence (points oranges) d'une espèce. 

![](tree.png)

Dans le cas du Random Forest, plusieurs arbres (des dizaines, centaines ou  miliers) sont construits/calibrés sur des sous-ensembles aléatoires du tableau de départ. C'est-à-dire que pour chaque arbre, une partie des lignes et une partie des colonnes seulement sont prises en compte. Un arbre décisionnel est donc calibré sur chacun des ces sous-ensembles, et le modèle final (la forêt) tient compte du résultat de tous les arbres. Sur la base d'une même série de valeurs environnementales, tous les arbres ne prédisent pas forcément le même résultat. Il y a donc un choix final basé sur le nombre de 'votes'. Imaginons un Random Forest ('alimenté' par une série de valeurs environnementales pour un point donné) dont 30% des arbres prédisent une présence et le reste une absence. On pourra dès lors considérer que l'espèce a une probabilité de 30% d'être présente. La technique de Random Forest permet de réduire l'erreur liée à la variance (le surapprentissage) par rapport à l'utlisation d'un seul arbre construit sur la base de toutes les observations (ex. technique appelée Recursive partitionning, dont la fonction dans R s'appelle rpart) (voir note plus haut sur le compromis entre le biais et la variance).

![](randomForest.png)

```{r RF}
# Calibration du Random Forest. Nous aimerions utiliser le mode 'classification', pour cela nous devons spécifier que la colonne VulpesVulpes est en factor (catégorie 1 et catégorie 0). L'autre mode possible est le mode 'regression', utilisé pour modéliser des valeurs continues.
modRF <- randomForest(as.factor(VulpesVulpes)~.,data=dataSpEnv)
# Summary
print(modRF)
```

La **matrice de confusion** affichée lorsque l'on exécute l'objet modRF nous indique le nombre de présences (1) et absences (0) correctement prédites (sur nos points d'observations utilisés pour calibrer notre modèle). L'idée est de comparer ce que l'on a observé et ce que l'on a prédit à l'aide du modèle. Ce type de matrice sera utile par la suite pour évaluer et comparer les performances de nos différents modèles.

![](confusMatrix.png)


### Partie III: Evaluation de l'importance relative de chaque variable explicative

Une méthode relativement simple qui permet d'évaluer l'importance d'une variable explicative (prédicteur) pour un modèle donné est d'observer la variation dans les prédictions lorsque l'on mélange de manière aléatoire les valeurs de la variable concernée. Si les prédictions ne varient pas ou peu, on considère que la variable en question n'est pas importante, en revanche si les prédictions varient beaucoup, alors on considère que la variable est importante. Pour quantifier cette importance, on calcule un coefficient de corrélation (cor) entre les prédictions initiales et les prédictions 'avec mélange', et on applique la formule suivante: 1-cor (à noter que n'importe quelle valeur de 'cor' négative sera considérée comme étant nulle; en effet, on considère que si la corrélation est négative, l'importance relative est maximale).
La valeur de l'importance relative d'une variable est donc comprise en 0 et 1. 

Comme exemple, nous allons quantifier l'importance relative de la variable bio7 pour notre modèle GLM:

```{r glmVarImp}
# PredI (I pour initial) correspond aux prédictions de notre modèle (sur nos points d'observations). Il est important de spécifier type="response" afin d'obtenir les valeurs après transformation grâce à la fonction lien (voir théorie GLM). Les valeurs prédites seront comprises entre 0 et 1.
predI <- predict(modGLM,type="response")
# Mélange de bio7: Afin de ne pas modifier notre tableau de base, nous allons en faire une copie appelée dataSpEnvM (M pour modifié).
dataSpEnvM <- dataSpEnv
# La fonction sample nous permet d'extraire un échantillon aléatoire. Dans notre cas, nous spécifions que la taille de l'échantillon doit correspondre au nombre de ligne de notre tableau et que chaque élément ne peut être séléctionné plus d'une fois (replace=FALSE). Ceci revient à 'mélanger' les valeurs de la colonne concernée.
dataSpEnvM$bio7 <- sample(dataSpEnvM$bio7,size=nrow(dataSpEnvM),replace=FALSE)
# Prédiction avec variable mélangée.
predM <- predict(modGLM,newdata=dataSpEnvM,type="response")
# Calcul de l'importance relative (la fonction max, permet de considérer n'importe quel coefficient négatif comme étant nulle).
varImp <- 1-max(cor(predM,predI),0)
print(varImp)
```

Le mélange, étant aléatoire, peut varier d'une fois à l'autre, c'est pourquoi il est important d'effectuer cette opération plusieurs fois et de calculer ensuite une moyenne des valeurs obtenues. Pour cela nous allons créer une boucle itérative. Pour une introduction sur le fonctionnement des boucles dans R ('loop' en anglais), visitez la page suivante: https://datascienceplus.com/how-to-write-the-loop-in-r/ 

```{r glmVarImpTimes}
# PredI (I pour initial) correspond aux prédictions de notre modèle (sur nos points d'observations). Il est important de spécifier type="response" afin d'obtenir les valeurs après transformation grâce à la fonction lien (voir théorie GLM). Les valeurs prédites seront comprises entre 0 et 1.
predI <- predict(modGLM,type="response")
# Nombre d'itérations.
nIter <- 10
# Vecteur vide dans lesquels nous allons sauvegarder les valeurs obtenues à chaque itération, dont la longeur correspond au nombre d'itérations.
varImpVect <- vector(mode="numeric",length=nIter) 
# Boucle (la valeur de i changera à chaque itération et ira de 1 au nombre défini dans nIter)
for(i in 1:nIter){
  # Mélange de bio7: Afin de ne pas modifier notre tableau de base, nous allons en faire une copie appelée dataSpEnvM (M pour modifié).
  dataSpEnvM <- dataSpEnv
  # La fonction sample nous permet d'extraire un échantillon aléatoire. Dans notre cas, nous spécifions que la taille de l'échantillon doit   correspondre au nombre de ligne de notre tableau et que chaque élément ne peut être séléctionner plus d'une fois. Ceci revient à 'mélanger' les valeurs de la colonne concernée.
  dataSpEnvM$bio7 <- sample(dataSpEnvM$bio7,size=nrow(dataSpEnvM),replace=FALSE)
  # Prédiction avec variable mélangée
  predM <- predict(modGLM,newdata=dataSpEnvM,type="response")
  # Calcul de l'importance relative (la fonction max, permet de considérer n'importe quel coefficient négatif comme étant nulle).
  varImp <- 1-max(cor(predM,predI),0)
  # Sauvegarde de la valeur dans l'objet varImpVect à la position i
  varImpVect[i] <- varImp
}
# Calcul de la moyenne de toutes les valeurs obtenues
varImpMoy <- mean(varImpVect)
print(varImpMoy)
```

Maintenant, nous allons appliquer le même principe pour tous les prédicteurs. Nous allons donc créer un tableau dont les lignes correspondront aux différentes itérations et les colonnes aux différents prédicteurs. Ce tableau sera rempli comme précédemment, de manière itérative, mais cette fois, nous allons avoir une double boucle (imbrication), une sur les prédicteurs, puis une autre sur les itérations.

```{r glmVarImpFinal}
# PredI (I pour initial) correspond aux prédictions de notre modèle (sur nos points d'observations). Il est important de spécifier type="response" afin d'obtenir les valeurs après transformation grâce à la fonction lien (voir théorie GLM). Les valeurs prédites seront comprises entre 0 et 1.
predI <- predict(modGLM,type="response")
# Nombre d'itérations.
nIter <- 10
# Tableau vide dans lequel nous allons sauvegarder les valeurs obtenues à chaque itération, dont les lignes correspondent aux itérations et les colonnes aux variables. Nous créons d'abord une matrice remplie de NA, pour laquelle il est facile de spécifier le nombre de lignes (nrow=) et le nombre de colonnes (ncol=). Nous transformons ensuite cette matrice en data.frame.
varImpVectTable <- data.frame(matrix(NA,nrow=nIter,ncol=length(preds)))
colnames(varImpVectTable) <- preds
# Boucle (la valeur de p changera à chaque itération et correspondra aux noms de variables contenues dans l'objet preds)
for(p in preds){
  for(i in 1:nIter){
  # Mélange de la variable p. Afin de ne pas modifier notre tableau de base, nous allons en faire une copie appelée dataSpEnvM (M pour modifié).
  dataSpEnvM <- dataSpEnv
  # La fonction sample nous permet d'extraire un échantillon aléatoire. Dans notre cas, nous spécifions que la taille de l'échantillon doit   correspondre au nombre de ligne de notre tableau et que chaque élément ne peut être séléctionner plus d'une fois. Ceci revient à 'mélanger' les valeurs de la colonne concernée. L'opération sera effectuée sur la colonne p.
  dataSpEnvM[,p] <- sample(dataSpEnvM[,p],size=nrow(dataSpEnvM),replace = FALSE)
  # Prédiction avec variable mélangée
  predM <- predict(modGLM,newdata=dataSpEnvM,type="response")
  # Calcul de l'importance relative (la fonction max, permet de considérer n'importe quel coefficient négatif comme étant nulle)
  varImp <- 1-max(cor(predM,predI),0)
  # Sauvegarde de la valeur dans le tableau, à la ligne i (numéro de l'itération) et à la colonne p (correspondante à une des variables)
  varImpVectTable[i,p] <- varImp
  }
}
# Calcul des moyennes avec la fonction apply (fonction qui s'opère sur un data frame). La valeur 2 signifie que l'on souhaite exécuter une opération par colonne, et 'mean' correspond à l'opération que l'on souhaite réaliser.
varImpMoy <- apply(varImpVectTable,2,mean)
# Barplot, ylim permet de fixer l'axe des y, ce que permet de pouvoir comparer les graphiques entre eux (GLM vs Random Forest).
barplot(varImpMoy, ylab="Relative variable importance", main="Vulpes vulpes (GLM)",ylim=c(0,1))
```

Nous voyons que les variables bio3 et bio7 sont largement plus importantes que les variables bio11 et bio12, cette dernière étant presque négligeable. En d'autres termes, les variables bio3 et bio7 ont permis de bien mieux discriminer les présences des absences que les variables bio11 et bio12. 

Nous allons effectuer exactement la même opération pour le modèle Random Forest.

```{r rfVarImpFinal}
# PredI (I pour initial) correspond aux prédictions de notre modèle (sur nos points d'observations). Nous sélectionnons la deuxième colonne qui correspond aux probabilités de présence. 
predI <- predict(modRF,type="prob")[,2]
# Nombre d'itérations
nIter <- 10
# Tableau vide dans lequel nous allons sauvegarder les valeurs obtenues à chaque itération, dont les lignes correspondent aux itérations et les colonnes aux variables. Nous créons d'abord une matrice remplie de NA, pour laquelle il est facile de spécifier le nombre de lignes (nrow=) et le nombre de colonnes (ncol=). Nous transformons ensuite cette matrice en data.frame.
varImpVectTable <- data.frame(matrix(NA,nrow=nIter,ncol=length(preds)))
colnames(varImpVectTable) <- preds
# Boucle (la valeur de p changera à chaque itération et correspondra aux chaînes de charactères contenues dans l'objet preds)
for(p in preds){
  for(i in 1:nIter){
  # Mélange de la variable p. Afin de ne pas modifier notre tableau de base, nous allons en faire une copie appelée dataSpEnvM (M pour modifié).
  dataSpEnvM <- dataSpEnv
  # La fonction sample nous permet d'extraire un échantillon aléatoire. Dans notre cas, nous spécifions que la taille de l'échantillon doit   correspondre au nombre de ligne de notre tableau et que chaque élément ne peut être séléctionner plus d'une fois. Ceci revient à 'mélanger' les valeurs de la colonne concernée. L'opération sera effectuée sur la colonne p.
  dataSpEnvM[,p] <- sample(dataSpEnvM[,p],size=nrow(dataSpEnvM),replace = FALSE)
  # Prédiction avec variable mélangée
  predM <- predict(modRF,newdata=dataSpEnvM,type="prob")[,2]
  # Calcul de l'importance relative (la fonction max, permet de considérer n'importe quel coefficient négatif comme étant nulle)
  varImp <- 1-max(cor(predM,predI),0)
  # Sauvegarde de la valeur dans le tableau, à la ligne i (numéro de l'itération) et à la colonne p (correspondante à une des variables)
  varImpVectTable[i,p] <- varImp
  }
}
# Calcul des moyennes avec la fonction apply (fonction qui s'opère sur un data frame). La valeur 2 signifie que l'on souhaite exécuter une opération par colonne, et 'mean' correspond à l'opération que l'on souhaite réaliser.
varImpMoy <- apply(varImpVectTable,2,mean)
# Barplot, ylim permet de fixer l'axe des y, ce que permet de pouvoir comparer les graphiques entre eux
barplot(varImpMoy, ylab="Relative variable importance", main="Vulpes vulpes (RF)",ylim=c(0,1))
```

Nous pouvons évaluer les différences selon la technique de modélisation utilisée. Avec le Random Forest, l'importance des variables est plus homogène qu'avec le GLM. Ceci peut s'expliquer par le fait que dans le Random Forest, des interactions complexes entre les variables peuvent être prises en compte, ce qui peut potentiellement équilibrer l'importance relative de chacune des variables.


### Partie IV: Evaluation des 'courbes réponses' associées à chaque variable explicative

L'idée ici sera de comprendre comment l'espèce répond (comment la probabilité de présence varie) le long des gradients des différents prédicteurs. Il existe là aussi une méthode relativement simple, quel que soit la technique de modélisation employée. Comme pour le calcul de l'importance des variables, l'idée est de modifier le tableau initial, prédire puis analyser les prédictions. Imaginons que nous souhaitions savoir comment *Vulpes vulpes* se comporte en fonction de la variable bio3. Nous allons d'un côté modifier les valeurs de bio3, en créant un gradient allant de la valeur minimum à la valeur maximum. Et d'un autre côté, nous allons 'fixer' toutes les valeurs des autres colonnes, de telle manière à ce que les autres variables n'affectent pas les prédictions. C'est-à-dire, la colonne de la variable bio7, par exemple, aura la même valeur à toutes les lignes. Cette valeur pourra correspondre à la moyenne, la médiane, la valeur minimum, etc. Nous pourrons dès lors inférer que la variation dans les nouvelles prédictions sera seulement causée par la variation dans la variable d'intérêt (dans notre exemple, la variable bio3). A noter que cette technique ne tient pas compte des possibles interactions entre les variables (ex. interactions modélisées par les arbres décisionnels).

Comme exemple, nous allons donc évaluer la courbe réponse de la variable bio3 pour notre modèle GLM:

```{r glmCurveResp}
# Afin de ne pas modifier notre tableau de base, nous allons en faire une copie appelée dataSpEnvM (M pour modifié).
dataSpEnvM <- dataSpEnv
# Boucle permettant de fixer les valeurs de chaque colonne (d'attribuer la même valeur à chaque ligne). A chaque itération, nous prenons un colonne p correspondant à un des prédicteur (preds) et nous attribuons la valeur médiane à toutes les lignes
for(p in preds){
  dataSpEnvM[,p] <- median(dataSpEnvM[,p])
}
# La fonction imbriquée suivante (en commentaire) est une méthode alternative à la boucle précédente (elle peut sembler plus compliquée mais elle est plus rapide et donne exactement le même résultat). La partie apply calcule la médiane par colonne, et la fonction sapply applique la fonction 'rep' au résultat de la fonction apply, à savoir, elle va répéter chacune de ces valeurs un nombre de fois qui correspond au nombre de lignes de notre tableau. 
# dataSpEnvM[,preds] <- sapply(apply(dataSpEnvM[,preds],2,median),rep,times=nrow(dataSpEnvM))

# Nous allons maintenant remplacer les valeurs de la colonne bio3 par des valeurs correspondant à un gradient allant de la valeur minimum à la valeur maximum, dont la longueur correspond aux nombre de lignes de notre tableau. Nous calculons ce gradient sur la base des valeurs initiales (dataSpEnv).
gradient <- seq(min(dataSpEnv$bio3),max(dataSpEnv$bio3),length.out = nrow(dataSpEnv))
dataSpEnvM$bio3 <- gradient
# Prédiction sur le tableau modifié.
predM <- predict(modGLM,newdata=dataSpEnvM,type="response")
# Plot de la courbe réponse. En 'x' le gradient, en 'y' les prédictions.
plot(x=dataSpEnvM$bio3,y=predM,type="l",ylab="Presence probability",xlab="bio3",main="Response curve (GLM)",col="blue",ylim=c(0,1))
```

Nous voyons ici l'effet de la variable bio3 le long de son gradient sur la probabilité de présence de notre espèce.

Nous allons appliquer le même principe pour toutes les variables et pour les deux techniques de modélisation. Nous allons programmer une boucle itérative nous permettant d'afficher toutes les courbes réponses pour chacun des techniques.

```{r glmCurveRespAll}
# Afin de ne pas modifier notre tableau de base, nous allons en faire une copie appelée dataSpEnvM (M pour modifié).
dataSpEnvM <- dataSpEnv
# Boucle permettant de fixer les valeurs de chaque colonne (d'attribuer la même valeur à chaque ligne). A chaque itération, nous prenons un colonne et nous attribuons la valeur médiane à toutes les lignes.
for(p in preds){
  dataSpEnvM[,p] <- median(dataSpEnvM[,p])
}
# La fonction imbriquée suivante (en commentaire) est une méthode alternative à la boucle précédente (elle est plus rapide et donne exactement le même résultat). La partie apply calcule la médiane par colonne, et la fonction sapply applique la fonction 'rep' au résultat de la fonction apply, à savoir, elle va répéter chacune de ces valeurs un nombre de fois qui correspond au nombre de lignes de notre tableau. 
# dataSpEnvM[,preds] <- sapply(apply(dataSpEnvM[,preds],2,median),rep,times=nrow(dataSpEnvM))

# Comme nous avons 4 variables, nous allons changer les paramètre graphiques de telle manière à pouvoir afficher sur la même fenêtre les 4 courbes réponses (2 lignes et 2 colonnes)
par(mfrow=c(2,2))
# Modèle GLM: boucle itérative sur les prédicteurs.
for(p in preds){
  # Afin de ne pas modifier notre tableau de médiane, nous allons en faire une copie appelée dataSpEnvMG (G pour gradient)
  dataSpEnvMG <- dataSpEnvM
  # Nous allons maintenant remplacer les valeurs de la colonne i par des valeurs correspondant à un gradient allant de la valeur minimum à la valeur maximum, dont la longueur correspond aux nombre de lignes de notre tableau. Nous calculons ce gradient sur la base des valeurs initiales (dataSpEnv).
  gradient <- seq(min(dataSpEnv[,p]),max(dataSpEnv[,p]),length.out = nrow(dataSpEnv))
  dataSpEnvMG[,p] <- gradient
  # Prédiction sur le tableau modifié
  predM <- predict(modGLM,newdata=dataSpEnvMG,type="response")
  # Plot de la courbe réponse. En 'x' le gradient, en 'y' les prédictions.
  plot(x=dataSpEnvMG[,p],y=predM,type="l",ylab="Presence probability",xlab=p,main="Response curve (GLM)",col="blue",ylim=c(0,1))
}
par(mfrow=c(2,2))
# Modèle RF: boucle itérative sur les prédicteurs.
for(p in preds){
  # Afin de ne pas modifier notre tableau de médiane, nous allons en faire une copie appelée dataSpEnvMG (G pour gradient)
  dataSpEnvMG <- dataSpEnvM
  # Nous allons maintenant remplacer les valeurs de la colonne i par des valeurs correspondant à un gradient allant de la valeur minimum à la valeur maximum, dont la longueur correspond aux nombre de lignes de notre tableau. Nous calculons ce gradient sur la base des valeurs initiales (dataSpEnv).
  gradient <- seq(min(dataSpEnv[,p]),max(dataSpEnv[,p]),length.out = nrow(dataSpEnv))
  dataSpEnvMG[,p] <- gradient
  # Prédiction sur le tableau modifié. Nous sélectionnons la deuxième colonne qui correspond aux probabilité de présence.
  predM <- predict(modRF,newdata=dataSpEnvMG,type="prob")[,2]
  # Plot de la courbe réponse. En 'x' le gradient, en 'y' les prédictions.
  plot(x=dataSpEnvMG[,p],y=predM,type="l",ylab="Presence probability",xlab=p,main="Response curve (RF)",col="blue",ylim=c(0,1))
}

```

Nous observons que les courbes réponses du Random Forest ne sont pas aussi lisses que celle du GLM. Ceci est dû au fait que les arbres décisionnels se forment sur la base de partition à partir de seuils (ex. température > 5°C). Nous observons donc des ruptures, parfois abruptes.


### Partie V: Evaluation de la performance prédictive des modèles

Après avoir abordé l'évaluation de l'importance des variables et des courbes réponses comme moyen d'interprétation des modèles d'un point de vue écologique, nous allons à présent voir comment mesurer la performance de ces modèles. Cela nous permettra de savoir si nos modèles sont fiables et/ou quelle technique entre le GLM et le Random Forest fonctionne le mieux dans une logique prédictive.

Comme nous avons vu plus haut (matrice de confusion), l'idée est de comparer les prédictions et les observations et de se poser la question suivante: "Est-ce que notre modèle prédit ce que l'on observe ?". Pour faire simple, Si la réponse est oui, le modèle est bon, si la réponse est non, le modèle est mauvais. Évidemment, nous parlons là de prédictions sur des points d'observations connus. Si le modèle est bon, alors on peut l'extrapoler et l'utiliser pour prédire à une plus grande échelle.  

Cependant, il existe un risque de surapprentissage, surtout avec des techniques permettant la calibration de modèles complexes telles que le Random Forest. Rappelons que le surapprentissage permet de très bien prédire sur les points d'observations utilisés pour la calibration (le bruit est pris en compte), mais les modèles ne sont pas extrapolables et les prédictions sur d'autres points ne sont donc pas fiables. La seule manière de détecter un potentiel surapprentissage est de tester nos prédictions sur des points qui n'ont pas été pris en compte lors de la calibration. L'idée est donc d'avoir une série de points d'observations pour la calibration, et une autre série pour l'évaluation. Avoir un jeu de données complètement indépendant qui ne serait pas utilisé pour calibrer nos modèles serait l'idéal, mais la plupart du temps, nous ne disposons pas d'un tel jeu de données. C'est pourquoi, une méthode assez répandue est de séparer notre tableau initial en deux parties. La première partie étant utilisée pour calibrer nos modèles, et la deuxième pour les tester/évaluer. Ceci correspond à une technique (parmi d'autres) de **validation croisée**. Nous allons voir comment procéder en prenant comme exemple le GLM.

![](split.png)

```{r glmCV}
# Nous allons utiliser la fonction sampleMat2 du package 'biomod2' qui permet de séparer de manière aléatoire un vecteur de présence-absence (binaire) de telle manière à maintenir la même prévalence (pourcentage de présence) dans les deux groupes. Un paramètre 'ratio' permet de définir la proportion d'observation que l'on souhaite garder pour la calibration. En règle générale, cette proportion va de 0.6 à 0.8. 
calibEval <- SampleMat2(dataSpEnv$VulpesVulpes,ratio = 0.7)
# L'objet issu de la fonction sampleMat2 est une liste de deux vecteurs. Le premier (appelé calibration) indique les numéros des lignes prévus pour la calibration et le deuxième (appelé evaluation), le numéro des lignes prévus pour le test. 
# On crée un tableau (déstiné à la partie calibration) avec les lignes du tableau initial indiquées par le vecteur calibEval$calibration
calib <- dataSpEnv[calibEval$calibration,]
# On crée un tableau (déstiné à la partie test) avec les lignes du tableau initial indiquées par le vecteur calibEval$evalution
eval <- dataSpEnv[calibEval$evaluation,]
# Calibration du GLM. Le nom de la variable réponse en fonction de toutes les variables (~.) contenues dans le tableau (data=). Nous spécifions aussi la famille de distribution (binomial) dont la fonction lien est la fonction 'logit'. Cette fois-ci, nous spécifions data=calib.
modGLM <- glm(VulpesVulpes~.,data=calib,family = binomial("logit"))
# Prédiction sur la partie test. Les valeurs environnementales du tableau 'eval' sont utilisées par le modèle pour prédire. Il est important de spécifier type="response" afin d'obtenir les valeurs après transformation grâce à la fonction lien (voir théorie GLM). Les valeurs prédites seront comprises entre 0 et 1.
predTest <- predict(modGLM,newdata=eval,type="response")
print(summary(predTest))
```

Les valeurs préditent sont comprisent en 0 et 1 (modèle binomial) et correspondent à des probabilités de présence. Comment comparer des probabilités à des observations binaires (0 ou 1) ? Car nous aimerions comparer nos prédictions aux observations et dans le jeu de données 'eval' issu du tableau initial, ce sont bien des données binaires que nous avons. Une solution consiste à transformer ces probabilités en binaire, selon un seuil. Par exemple, d'une manière arbitraire, nous pouvons décider que tout ce qui est au-dessus de 0.5 devient 1 et ce qui est dessous devient 0.

```{r glmBin}
# Avec la fonction ifelse, nous créons un vecteur binaire sur la base des probabilités prédites (sur la partie test). La fonction requiert trois éléments: une condition, le résultat si la condition est remplie, le résultat si la condition n'est pas remplie.
bin <- ifelse(predTest>=0.5,1,0)
```

Nous avons là donc deux vecteurs binaires, celui des observations et celui des prédictions, et nous serions donc capables de remplir la matrice de confusion. Plusieurs indices peuvent être calculés sur la base de cette matrice, par exemple la sensibilité (pourcentage de présences correctement prédites), la spécificité (pourcentage d'absences correctement prédites), ou encore des indices composés tel que l'indice True Skill Statistics (TSS) qui n'est autre que la somme de la sensibilité et de la spécificité moins 1 (SE+SP-1). 

```{r glmTSS05}
# La fonction Find.Optim.Stat nous permet de calculer plusieurs indices de performance prédictive d'un modèle. Pour cela, il faut une vecteur de prédiction (en probabilités) et un vecteur d'observation (binaire). En l'occurence, nous avons décidé de fixer le seuil à 0.5, et la fonction s'occupera de transformer nos probabilités en valeurs binaires (comme nous l'avont fait précédemment).
tss <- Find.Optim.Stat(Stat="TSS",predTest,eval$VulpesVulpes,Fixed.thresh = 0.5)
print(tss)
```

Le résultat de cette fonction nous informe sur la valeur de l'indice TSS (best.stat), le seuil utilisé (cutoff), la sensibilité et la spécificité.
Pour cet indice, une valeur entre 0.5 et 0.7 indique que le modèle est 'bon', entre 0.7 et 0.85 'très bon', et au-delà de 0.85 il est considéré 'excellent'. 

Dans cet exemple, nous avons défini un seuil de manière arbitraire, mais d'autres seuils pourraient potentiellement améliorer notre indice. La même fonction permet de tester différents seuils.

```{r glmTSS}
# La fonction Find.Optim.Stat nous permet de calculer plusieurs indices de performance prédictive d'un modèle. Pour cela, il faut une vecteur de prédiction (en probabilités) et un vecteur d'observation (binaire). Contrairement à l'étape précédente, nous allons demander à la fonction de tester différents seuils, allant de 0 à 1 avec un pas de 0.01 .
tss <- Find.Optim.Stat(Stat="TSS",predTest,eval$VulpesVulpes,Fixed.thresh = seq(0,1,by = 0.01))
print(tss)
```

Nous avons là l'indication du seuil qui maximisme cet indice (cutoff) et la valeur de l'indice en tenant compte de ce seuil (best.stat).

Un autre indice fréquemment utilisé est l'AUC (Area Under the Curve) basée sur le ROC (Relative Operating Characteristic). Au-delà de ces termes techniques, le principe est relativement simple. On place deux axes, le taux de faux positifs (égal au pourcentage d'absences mal prédites) sur l'axe des X et le taux de vrais positifs (pourcentage de présences bien prédites) sur l'axe des Y. Chaque seuil donné correspondra à un point différent sur le graphique. Par exemple, le seuil de 0 (une probabilité supérieure à 0 devient 1; 100% de présence) rendra le taux de faux positifs ainsi que le taux de vrais positifs maximums. Le point serait donc placé dans le coin en haut à droite du graphique. En effet, toutes les absences seront codées comme étant des présences (taux de faux positifs = 1), mais toutes les présences seront codées comme étant présentes (taux de vrais positifs = 1). A l'inverse, un seuil de 1 éliminerait les faux positifs mais il n'y aurait pas non plus de vrais positifs. La courbe sera donc celle qui reliera tous les points obtenus selon tous les seuils possibles. Et plus la courbe se rapprochera de la prédiction parfaite (taux de faux positifs de 0 et taux de vrais positifs de 1), plus l'aire sous la courbe (AUC) sera proche de 1, plus notre modèle sera considéré comme étant bon. La même fonction Find.Optim.Stat permet d'obtenir la valeur de l'AUC. Le 'cutoff' indiqué dans ce cas sera celui qui maximise la sensibilité et la spécifité, mais il n'a pas d'influence sur l'AUC, qui est calculé en tenant compte de tous les seuils.  

![](roc.png)

```{r glmAUC, warning=FALSE}
# La fonction Find.Optim.Stat nous permet de calculer l'AUC (appelé ROC dans le cas de cette fonction). 
auc <- Find.Optim.Stat(Stat = "ROC",predTest,eval$VulpesVulpes)
print(auc)
```

La séparation en deux parties de notre tableau initial se fait de manière aléatoire. Les valeurs de TSS, d'AUC ou d'autres indices peuvent donc varier. Il est dès lors important de répéter la même opération plusieurs fois afin d'observer une tendance fiable. Nous allons donc effectuer un 'split' (une partition) une dizaine de fois. A chaque itération nous allons calibrer un GLM et un Random Forest puis nous allons calculer les valeurs de TSS. Les résultats seront sauvegardés dans un tableau.

```{r glmRfAUC, warning=FALSE}
# Nombre d'itérations
nIter <- 10
# Création d'un tableau dont les lignes correspondent au nombre d'itération et les colonnes aux deux techniques, GLM et RF. Nous créons d'abord une matrice remplie de NA, pour laquelle il est facile de spécifier le nombre de lignes (nrow=) et le nombre de colonnes (ncol=). Nous transformons ensuite cette matrice en data.frame.
tssTable <- data.frame(matrix(NA,nrow=nIter,ncol=2))
# On nomme les colonnes.
colnames(tssTable) <- c("GLM","RF")
# Itération (i prendra la valeur de l'itération, de 1 au nombre indiqué plus haut).
for(i in 1:nIter){
  # Nous allons utiliser la fonction sampleMat2 du package 'biomod2' qui permet de séparer de manière aléatoire un vecteur de présence-absence     (binaire) de telle manière à maintenir la même prévalence (pourcentage de présence) dans les deux groupes. Un paramètre 'ratio' permet de définir la proportion d'observation que l'on souhaite garder pour la calibration. En règle générale, cette proportion va de 0.6 à 0.8. 
  calibEval <- SampleMat2(dataSpEnv$VulpesVulpes,ratio = 0.7)
  # L'objet issu de la fonction sampleMat2 est une liste de deux vecteurs. Le premier (appelé calibration) indique les numéros des lignes prévus pour la calibration et le deuxième (appelé evaluation), le numéro des lignes prévus pour le test.
  # On crée un tableau (déstiné à la partie calibration) avec les lignes du tableau initial indiquées par le vecteur calibEval$calibration.
  calib <- dataSpEnv[calibEval$calibration,]
  # On crée un tableau (déstiné à la partie test) avec les lignes du tableau initial indiquées par le vecteur calibEval$evalution.
  eval <- dataSpEnv[calibEval$evaluation,]
  
  # GLM
  # Calibration du GLM. Le nom de la variable réponse en fonction de toutes les variables (~.) contenues dans le tableau (data=). Nous spécifions aussi la famille de distribution (binomial) dont la fonction lien est la fonction 'logit'. Cette fois-ci, nous spécifions data=calib.
  modGLM <- glm(VulpesVulpes~.,data=calib,family = binomial("logit"))
  # Prédiction sur la partie test. Les valeurs environnementales du tableau 'eval' sont utilisées par le modèle pour prédire. Il est important de spécifier type="response" afin d'obtenir les valeurs après transformation grâce à la fonction lien (voir théorie GLM). Les valeurs prédites seront comprises entre 0 et 1.
  predTest <- predict(modGLM,newdata=eval,type="response")
  # Calcul du TSS comme vu précédemment. Nous sauvegardons le premier élément [1] de l'objet issu de la fonction Find.Optim.Stat qui correspond à la valeur du TSS.
  tss <- Find.Optim.Stat(Stat = "TSS",predTest,eval$VulpesVulpes,Fixed.thresh = seq(0,1,by = 0.01))[1]
  # Nous sauvegardons la valeur de l'objet tss à la ligne i qui correspond un numéro de l'itération en cours et à la colonne "GLM"
  tssTable[i,"GLM"] <- tss
  
  # Random Forest
  # Calibration du Random Forest. Nous aimerions utiliser le mode 'classification', pour cela nous devons spécifier que la colonne VulpesVulpes est en factor (catégorie 1 et catégorie 0). Nous calibrons notre modèle sur le jeu de données 'calib'.
  modRF <- randomForest(as.factor(VulpesVulpes)~.,data=calib)
  # Prédiction sur la partie test. Les valeurs environnementales du tableau 'eval' sont utilisées par le modèle pour prédire. Nous sélectionnons la deuxième colonne qui correspond aux probabilité de présence.
  predTest <- predict(modRF,newdata=eval,type="prob")[,2]
  # Calcul du TSS comme vu précédemment. Nous sauvegardons le premier élément [1] de l'objet issu de la fonction Find.Optim.Stat qui correspond à la valeur du TSS.
  tss <- Find.Optim.Stat(Stat = "TSS",predTest,eval$VulpesVulpes,Fixed.thresh = seq(0,1,by = 0.01))[1]
  # Nous sauvegardons la valeur de l'objet tss à la ligne i qui correspond un numéro de l'itération en cours et à la colonne "RF"
  tssTable[i,"RF"] <- tss
}

```

Nous pouvons dès lors comparer les valeurs de TSS (validation croisée) obtenues selon les deux techniques.

```{r boxplotAUC}
# Distribution des valeurs selon la technique.
boxplot(tssTable$GLM,tssTable$RF,names=c("GLM","RF"),ylab="TSS")
# Test statistique permettant de montrer que les deux techniques donnent des valeurs de TSS significativement différentes (si p-value < 0.05).
print(wilcox.test(tssTable$GLM,tssTable$RF))
```

Selon les valeurs de TSS obtenues, nous pouvons conclure, dans notre cas, que les deux techniques donnent de très bons résultats. Cependant, le Random Forest semble supérieur en termes de performance prédictive avec un TSS médian très élevé.


### Partie VI: Prédictions à l'échelle globale et conclusion

Nous allons maintenant utiliser nos modèles pour prédire à l'échelle du globe, en utilisant la fonction 'predict' du package raster. Cette fonction permet de prédire des raster pour autant que les noms des rasters en entrées (variables environnementales) contiennent les noms des variables utilisées pour la calibration du modèle.

```{r predictGLM}
# Noms des rasters en entrée. Ils contiennent bien les noms de nos prédicteurs utilisés pour calibrer nos modèles (objet preds).
names(dataEnv)
  # Calibration du GLM sur toutes nos données. Le nom de la variable réponse en fonction de toutes les variables (~.) contenues dans le tableau (data=). Nous spécifions aussi la famille de distribution (binomial) dont la fonction lien est la fonction 'logit'.
modGLM <- glm(VulpesVulpes~.,data=dataSpEnv,family = binomial)
# Prediction avec notre GLM. Il est important de spécifier type="response" afin d'obtenir les valeurs après transformation grâce à la fonction lien (voir théorie GLM). Les valeurs prédites seront comprises entre 0 et 1.
# Nous utilisons la syntaxe raster::predict pour s'assurer que R utilise bien la fonction predict du package 'raster'. Rapellons que dataEnv correspond à la pile de raster qui correspond aux variables environnementales et qui couvre toute la planète
predictGLM <- raster::predict(dataEnv,modGLM,type="response")
# Afficher raster
plot(predictGLM,main="Vulpes vulpes (GLM)")
```

Comme précédemment, utilisons notre modèle pour prédire à l'échelle du globe.

```{r predictRF}
# Calibration du RF sur toutes nos données. Nous aimerions utiliser le mode 'classification', pour cela nous devons spécifier que la colonne VulpesVulpes est en factor (catégorie 1 et catégorie 0).
modRF <- randomForest(as.factor(VulpesVulpes)~.,data=dataSpEnv)
# Prediction avec notre RF. Il est important de spécifier type="prob" afin d'obtenir des valeurs de probabilités et non pas binaires. Le paramètre index=2 permet d'obtenir la probabilité de présence, index=1 donnerait la probabilité d'absence.
predictRF <- raster::predict(dataEnv,modRF,type="prob",index=2)
plot(predictRF,main="Vulpes vulpes (RF)")
```

En comparant ces deux cartes, on remarque des différences significatives, principalement en Afrique du Nord, en Amérique du Sud et en Australie. 

Sur la base de notre évaluation de la performance prédictive, on pourrait dire que la carte obtenue grâce au modèle Random Forest est plus fiable. En revanche, l'interprétation d'un point de vue écologique des courbes réponses issues du Random Forest est plus difficile. Chaque technique à ses avantages et ses inconvénients. Il existe une multitude de techniques (GLM, GAM, MaxENT, Random Forest, Gradient Boosted Trees, SVC, etc.). Une tendance récente est de combiner plusieurs techniques pour créer ce que l'on appelle des 'modèles d'ensemble'. Dans notre cas, il serait par exemple possible de calculer une moyenne pondérée (selon les valeurs de TSS ou d'AUC) des prédictions du GLM et du Random Forest. Il existe aussi souvent plusieurs manières de paramétrer une technique. Par exemple, nous avons calibré un GLM simple, mais il nous aurait été possible d'ajouter un effet quadratique ou des interactions entre les variables. Par ailleurs, nous avons aussi toute une panoplie de méthodes permettant l'évaluation de la performance prédictive d'un modèle (différents indices, mais aussi différentes manières de partitionner notre jeu de données). En général, les choix dépendent principalement des objectifs, des données à disposition, mais aussi des compétences et habitudes de chacun.

Vous avez terminé ce TP !
